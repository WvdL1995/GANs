{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed=99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for i, layer in enumerate(nn_architecture):\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        params_values[\"W\" + str(i+1)] = np.random.randn(layer_output_size,layer_input_size) * 0.1\n",
    "        params_values[\"b\" + str(i+1)] = np.random.randn(layer_output_size,1) * 0.1  \n",
    "    return params_values\n",
    "\n",
    "nn_architecture_gen = [\n",
    "    {\"input_dim\": 1, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "nn_architecture_disc = [\n",
    "    {\"input_dim\": 1, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA,Z):\n",
    "    sig=sigmoid(Z)\n",
    "    return dA*sig*(1-sig)\n",
    "def relu_backward(dA,Z):\n",
    "    dZ=np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ\n",
    "\n",
    "def sing_forward_prop(A_prev, W_curr, b_curr,activation=\"relu\"):\n",
    "    print(np.size(W_curr))\n",
    "    print(np.size(A_prev))\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('activation function not found')\n",
    "    #print (activation_func(Z_curr))\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "def forward_prop(X,params_values,nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for i, layer in enumerate(nn_architecture):\n",
    "        layer_idx = i+1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr , Z_curr = sing_forward_prop(A_prev,W_curr,b_curr,activ_function_curr)\n",
    "\n",
    "        memory[\"A\" + str(i)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "    return A_curr, memory\n",
    "\n",
    "def sing_back_prop(dA_curr , W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6d4afef8aa9b79403bf825936c878d6b8a951d99c592db7f1cc1f2ac2c037a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
